{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naver_crawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcNPKuA90a-S"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFYWg-jWeoVi"
      },
      "source": [
        "def Naver_news_search_crawl(keyword, date_start, date_end, sub=True):\r\n",
        "  main_url = \"https://search.naver.com/search.naver?where=news&sm=tab_jum&query=\" + keyword + \"&nso=so%3Ar%2Cp%3Afrom\" + date_start +\"to\" + date_end\r\n",
        "  main_page = BeautifulSoup(requests.get(main_url).text, 'lxml')\r\n",
        "\r\n",
        "  news = []\r\n",
        "  while True:\r\n",
        "    article = main_page.find_all('li')\r\n",
        "\r\n",
        "    for a in article:\r\n",
        "      if a.find('a', {'class':'news_tit'}):\r\n",
        "        news.append(a.find_all('a', {'class':'news_tit'}))\r\n",
        "        if sub==True:\r\n",
        "          if a.find('a', {'class':'elss sub_tit'}):\r\n",
        "            news.append(a.find_all('a', {'class':'elss sub_tit'}))\r\n",
        "\r\n",
        "    if main_page.find('a', {'class':'btn_next'})['aria-disabled']=='true':\r\n",
        "      break\r\n",
        "\r\n",
        "    main_url = \"https://search.naver.com/search.naver?\" + main_page.find('a', {'class':'btn_next'})['href']\r\n",
        "    main_page = BeautifulSoup(requests.get(main_url).text, 'lxml')\r\n",
        "  \r\n",
        "  news = [n for N in news for n in N]\r\n",
        "\r\n",
        "  news_url = [n['href'] for n in news]\r\n",
        "\r\n",
        "  news_title = [n.text for n in news]\r\n",
        "\r\n",
        "  # save\r\n",
        "  data = pd.DataFrame({\r\n",
        "          \"title\": news_title,\r\n",
        "          \"url\": news_url\r\n",
        "          })\r\n",
        "  \r\n",
        "  if sub==True:\r\n",
        "    data.to_csv(keyword + '_' + date_start + '_' + date_end + '.csv',index=True)\r\n",
        "  else:\r\n",
        "    data.to_csv(keyword + '_' + date_start + '_' + date_end + '_main.csv',index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peemS-xnj0b3"
      },
      "source": [
        "def Naver_RankingNews_crawl(date):\r\n",
        "  # main page\r\n",
        "  main_url = \"https://news.naver.com/main/ranking/popularDay.nhn?date=\" + date\r\n",
        "  main_page = BeautifulSoup(requests.get(main_url, headers={'User-Agent':'Mozilla/5.0'}).text, 'html.parser')\r\n",
        "\r\n",
        "  news = main_page.find_all('div', {'class':'list_content'})\r\n",
        "\r\n",
        "  news_url = ['https://news.naver.com' + n.find('a')['href'] for n in news]\r\n",
        "  news_title = [n.find('a').text for n in news]\r\n",
        "  \r\n",
        "  news_content = []\r\n",
        "  for url in news_url:\r\n",
        "    # page\r\n",
        "    article_url = url\r\n",
        "    article_page = BeautifulSoup(requests.get(article_url, headers={'User-Agent':'Mozilla/5.0'}).text, 'html.parser')\r\n",
        "\r\n",
        "    # content\r\n",
        "    article = article_page.find('div', {'id':'articleBodyContents'}).text.replace('\\n', ' ')\r\n",
        "    content = article.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\r\n",
        "    news_content.append(content)\r\n",
        "\r\n",
        "    # comment\r\n",
        "    comment_url = article_url.replace('ranking/read.nhn?', 'read.nhn?m_view=1&includeAllCount=true&')\r\n",
        "    comment_page = BeautifulSoup(requests.get(comment_url, headers={'User-Agent':'Mozilla/5.0'}).text, 'html.parser')\r\n",
        "\r\n",
        "  # save\r\n",
        "  data = pd.DataFrame({\r\n",
        "          \"title\": news_title,\r\n",
        "          \"content\": news_content\r\n",
        "          })\r\n",
        "  \r\n",
        "  data.to_csv(date + '_Naver_RankingNews.csv',index=True)\r\n",
        "\r\n",
        "Naver_RankingNews_crawl('20210130')"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}