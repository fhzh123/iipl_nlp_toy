{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 파일 위치 탐색\nos.listdir('../input/quora-insincere-questions-classification')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# file unzip\n!unzip ../input/quora-insincere-questions-classification/embeddings.zip -d embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('./embeddings')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ndf = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torchtext로 인한 경고 무시\nimport warnings\nwarnings.filterwarnings(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchtext\nfrom nltk import word_tokenize\n\n# 필드 정의\ntext = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize, fix_length=70)\nqid = torchtext.data.Field()\ntarget = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n\n# 데이터셋 만들기\ntrain = torchtext.data.TabularDataset(path='../input/quora-insincere-questions-classification/train.csv', format='csv',\n                                      fields={'question_text': ('text',text),\n                                              'target': ('target',target)})\ntest = torchtext.data.TabularDataset(path='../input/quora-insincere-questions-classification/test.csv', format='csv',\n                                     fields={'qid': ('qid', qid),\n                                             'question_text': ('text', text)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 단어 집합 생성\ntext.build_vocab(train, test, min_freq=3) #min_freq : 단어 집합에 추가 시 단어의 최소 등장 빈도 조건 추가\nqid.build_vocab(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 사전 훈련된 단어 임베딩 로드\nfrom tqdm import tqdm, tqdm_notebook\n\nglove = torchtext.vocab.Vectors('./embeddings/glove.840B.300d/glove.840B.300d.txt')\ntqdm_notebook().pandas() # 프로그래스바","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.vocab.set_vectors(glove.stoi, glove.vectors, dim = 300)\n# tensor 컬렉션에서 vocab 인스턴스에 대한 벡터 설정\n# 문자열 사전, 벡터, 차원","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(1234)\nbatch_size = 512\ntrain_iter = torchtext.data.BucketIterator(dataset=train,\n                                               batch_size=batch_size,\n                                               shuffle=True,\n                                               sort=False)\n# 모든 텍스트 작업을 일괄로 처리하고 단어를 인덱스 숫자로 변환","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\n\nclass TextCNN(nn.Module):\n    def __init__(self, lm, padding_idx, static=True, kernel_num=128, fixed_length=50, kernel_size=[2, 5, 10], dropout=0.2):\n        super(TextCNN, self).__init__()\n        \n        self.dropout = nn.Dropout(dropout)\n        self.embedding = nn.Embedding.from_pretrained(lm)\n        \n        if static:\n            self.embedding.weight.requires_grad = False\n            \n        self.embedding.padding_idx = padding_idx\n        self.conv = nn.ModuleList([nn.Conv1d(1, kernel_num, (i, self.embedding.embedding_dim)) for i in kernel_size])\n        self.maxpools = [nn.MaxPool2d((fixed_length+1-i,1)) for i in kernel_size]\n        self.fc = nn.Linear(len(kernel_size)*kernel_num, 1)\n        \n    def forward(self, input):\n        x = self.embedding(input).unsqueeze(1)  # B X Ci X H X W\n        x = [self.maxpools[i](torch.tanh(cov(x))).squeeze(3).squeeze(2) for i, cov in enumerate(self.conv)]  # B X Kn\n        x = torch.cat(x, dim=1)  # B X Kn * len(Kz)\n        y = self.fc(self.dropout(x))\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search_best_f1(true, pred):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in np.arange(0.1, 0.501, 0.01):\n        tmp[1] = f1_score(true, np.array(pred)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    return tmp[2], delta\n\ndef training(epoch, model, loss_func, optimizer, train_iter):\n    e = 0\n    \n    while e < epoch:\n        train_iter.init_epoch()\n        losses, preds, true = [], [], []\n        for train_batch in tqdm(list(iter(train_iter)), 'epcoh {} training'.format(e)):\n            model.train()\n            x = train_batch.text.cuda()\n            y = train_batch.target.type(torch.Tensor).cuda()\n            true.append(train_batch.target.numpy())\n            model.zero_grad()\n            pred = model.forward(x).view(-1)\n            loss = loss_function(pred, y)\n            preds.append(torch.sigmoid(pred).cpu().data.numpy())\n            losses.append(loss.cpu().data.numpy())\n            loss.backward()\n            optimizer.step()\n        train_f1, alpha_train = search_best_f1([j for i in true for j in i], [j for i in preds for j in i])\n        print('epcoh {:02} - train_loss {:.4f} - train f1 {:.4f} - delta {:.4f}'.format(\n                            e, np.mean(losses), train_f1, alpha_train))\n                \n        e += 1\n    return alpha_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_network(model, method='xavier', exclude='embedding', seed=123):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    for name, w in model.named_parameters():\n        if not exclude in name:\n            if 'weight' in name:\n                if method is 'xavier':\n                    nn.init.xavier_normal_(w)\n                elif method is 'kaiming':\n                    nn.init.kaiming_normal_(w)\n                else:\n                    nn.init.normal_(w)\n            elif 'bias' in name:\n                nn.init.constant_(w, 0.0)\n            else: \n                pass\n\ndef print_model(model, ignore='embedding'):\n    total = 0\n    for name, w in model.named_parameters():\n        if not ignore or ignore not in name:\n            total += w.nelement()\n            print('{} : {}  {} parameters'.format(name, w.shape, w.nelement()))\n    print('-------'*4)\n    print('Total {} parameters'.format(total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import optim\n\ntext.fix_length = 70\nmodel = TextCNN(text.vocab.vectors, padding_idx=text.vocab.stoi[text.pad_token], kernel_size=[1, 2, 3, 5], kernel_num=128, static=False, fixed_length=text.fix_length, dropout=0.2).cuda()\ninit_network(model)\noptimizer = optim.Adam(params=model.parameters(), lr=1e-3)\nloss_function = nn.BCEWithLogitsLoss()\nprint_model(model, ignore=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nalpha = training(3, model, loss_function, optimizer, train_iter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, test_list):\n    pred = []\n    with torch.no_grad():\n        for test_batch in test_list:\n            model.eval()\n            x = test_batch.text.cuda()\n            pred += torch.sigmoid(model.forward(x).view(-1)).cpu().data.numpy().tolist()\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_list = list(torchtext.data.BucketIterator(dataset=test,\n                                    batch_size=batch_size,\n                                    sort=False,\n                                    train=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = predict(model, test_list)\nsub = pd.DataFrame()\nsub['qid'] = [qid.vocab.itos[j] for i in test_list for j in i.qid.view(-1).numpy()]\nsub['prediction'] = (preds > alpha).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}